{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bba2808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.metrics import Recall, Precision, CategoricalAccuracy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives, Accuracy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "# Próba importu tensorflow dla prawidłowego działania\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: TensorFlow nie jest zainstalowany. Zainstaluj wersje TensorFlow - 2.20.0\")\n",
    "\n",
    "\n",
    "directory_train = 'BrainTumorDataset/train'\n",
    "directory_test = 'BrainTumorDataset/test'\n",
    "\n",
    "no_tumor_train = os.path.join(directory_train, 'no')\n",
    "yes_tumor_train = os.path.join(directory_train, 'yes')\n",
    "no_tumor_test = os.path.join(directory_test, 'no')\n",
    "yes_tumor_test = os.path.join(directory_test, 'yes')\n",
    "\n",
    "\n",
    "print('Zbiór uczący')\n",
    "print(f'Liczba zdjęć (no tumor): {len(os.listdir(no_tumor_train))}')\n",
    "print(f'Liczba zdjęć (yes tumor): {len(os.listdir(yes_tumor_train))}')\n",
    "\n",
    "print('\\nZbiór testowy')\n",
    "print(f'Liczba zdjęć (no tumor): {len(os.listdir(no_tumor_test))}')\n",
    "print(f'Liczba zdjęć (yes tumor): {len(os.listdir(yes_tumor_test))}')\n",
    "\n",
    "# Tworzenie wykresu\n",
    "data_summary = [\n",
    "    {'subset': 'train', 'class': 'no',  'count': len(os.listdir(no_tumor_train))},\n",
    "    {'subset': 'train', 'class': 'yes', 'count': len(os.listdir(yes_tumor_train))},\n",
    "    {'subset': 'test',  'class': 'no',  'count': len(os.listdir(no_tumor_test))},\n",
    "    {'subset': 'test',  'class': 'yes', 'count': len(os.listdir(yes_tumor_test))}\n",
    "]\n",
    "df = pd.DataFrame(data_summary)\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "ax = sns.barplot(data = df, x = 'subset', y = 'count', hue = 'class', palette = 'viridis')\n",
    "\n",
    "plt.title('Liczebność klas w zbiorze danych', fontsize = 14)\n",
    "plt.ylabel('Liczba zdjęć')\n",
    "plt.xlabel('Podzbiór danych')\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f962d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "print(\"--- Wczytywanie zbioru treningowego i walidacyjnego ---\")\n",
    "train_dataset, valid_dataset = image_dataset_from_directory(\n",
    "    directory_train,\n",
    "    validation_split = 0.2, # 20% walidacji zdjęć - ochrona przed overfittingiem\n",
    "    subset = \"both\",\n",
    "    seed = 1337,\n",
    "    image_size = (256, 256),\n",
    "    batch_size = 32,\n",
    "    label_mode = 'binary',\n",
    "    color_mode = 'rgb'\n",
    ")\n",
    "\n",
    "print(\"\\n--- Wczytywanie zbioru testowego ---\")\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    directory_test,\n",
    "    seed = 1337,\n",
    "    image_size = (256, 256),\n",
    "    batch_size = 32,\n",
    "    label_mode = 'binary',\n",
    "    color_mode = 'rgb'\n",
    ")\n",
    "\n",
    "class_names = train_dataset.class_names\n",
    "print(f\"\\nZnalezione klasy: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8bdf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class_names = train_dataset.class_names\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(16):\n",
    "        ax = plt.subplot(4, 4, i + 1)\n",
    "        img_array = images[i].numpy().astype(\"uint8\").squeeze()\n",
    "        plt.imshow(img_array, cmap='gray')\n",
    "        plt.title(class_names[int(labels[i])])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ae58de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Sequential\n",
    "from tensorflow.keras.metrics import Recall\n",
    "\n",
    "\n",
    "def build_model_1_simple():\n",
    "    model = Sequential([\n",
    "        layers.Rescaling(1./255, input_shape=(256, 256, 3)),\n",
    "        layers.Conv2D(16, 3, activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(32, 3, activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ], name=\"Model_1_simple\")\n",
    "    return model\n",
    "\n",
    "def build_model_2_deep_dropout():\n",
    "    model = Sequential([\n",
    "        layers.Rescaling(1./255, input_shape=(256, 256, 3)),\n",
    "        layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ], name=\"Model_2_deep_dropout\")\n",
    "    return model\n",
    "\n",
    "def build_model_3_complex():\n",
    "    model = Sequential([\n",
    "        layers.Rescaling(1./255, input_shape=(256, 256, 3)),\n",
    "        layers.Conv2D(32, 3, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(64, 3, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(128, 3, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ], name=\"Model_3_Complex\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# trening\n",
    "models_to_train = [\n",
    "    build_model_1_simple(),\n",
    "    build_model_2_deep_dropout(),\n",
    "    build_model_3_complex()\n",
    "]\n",
    "\n",
    "history_dict = {}\n",
    "\n",
    "\n",
    "for model in models_to_train:\n",
    "    print(f\"\\n>>> Trenowanie: {model.name}...\")\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', Recall(name='recall')])\n",
    "    \n",
    "    # Trenowanie na danych z train_dataset\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=valid_dataset,\n",
    "        epochs=15, \n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    history_dict[model.name] = history\n",
    "\n",
    "print(\"\\n--- Zakończono wszystkie eksperymenty ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f92419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Wykres Dokładności\n",
    "plt.subplot(1, 2, 1)\n",
    "for model_name, history in history_dict.items():\n",
    "    plt.plot(history.history['val_accuracy'], linestyle='--', label=f'{model_name}')\n",
    "\n",
    "plt.title('Porównanie Dokładności (Validation Accuracy)')\n",
    "plt.xlabel('Epoka')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Wykres Straty\n",
    "plt.subplot(1, 2, 2)\n",
    "for model_name, history in history_dict.items():\n",
    "    plt.plot(history.history['val_loss'], linestyle='--', label=f'{model_name}')\n",
    "\n",
    "plt.title('Porównanie Straty (Validation Loss)')\n",
    "plt.xlabel('Epoka')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04debb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, recall_score, f1_score, balanced_accuracy_score # <--- DODANO IMPORT\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_model_index = 0\n",
    "best_model_name = \"\"\n",
    "\n",
    "print(\"--- RANKING MODELI ---\")\n",
    "\n",
    "for i, (name, history) in enumerate(history_dict.items()):\n",
    "    acc_key = 'val_accuracy' if 'val_accuracy' in history.history else 'val_acc'\n",
    "    \n",
    "    max_acc = max(history.history[acc_key])\n",
    "    print(f\"{i+1}. {name}: {max_acc*100:.2f}%\")\n",
    "    \n",
    "    if max_acc > best_val_acc:\n",
    "        best_val_acc = max_acc\n",
    "        best_model_index = i\n",
    "        best_model_name = name\n",
    "\n",
    "best_model = models_to_train[best_model_index]\n",
    "\n",
    "print(f\"\\n ZWYCIĘZCA: {best_model_name} (Dokładność walidacji: {best_val_acc*100:.2f}%)\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Generowanie macierzy pomyłek dla modelu: {best_model.name}...\")\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "    y_true.extend(labels.numpy().flatten())\n",
    "    predictions = best_model.predict(images, verbose=0)\n",
    "    y_pred.extend((predictions > 0.5).astype(int).flatten())\n",
    "\n",
    "# Macierz pomyłek\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Tumor', 'Tumor'], \n",
    "            yticklabels=['No Tumor', 'Tumor'])\n",
    "plt.title(f'Macierz Pomyłek - {best_model.name}')\n",
    "plt.ylabel('Prawdziwa klasa')\n",
    "plt.xlabel('Przewidziana klasa')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- RAPORT KOŃCOWY ---\")\n",
    "print(classification_report(y_true, y_pred, target_names=['No Tumor', 'Tumor']))\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "rec = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "bac = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Recall:   {rec:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"BAC:      {bac:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
